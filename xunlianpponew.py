import os
import sys
import time
import datetime
import subprocess
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import traci
from tqdm import tqdm
import matplotlib.pyplot as plt

# ========== é…ç½®åŒº ==========
class Config:
    # SUMOé…ç½®
    sumo_binary = "sumo"  # å¦‚æœå·²è®¾ç½®SUMO_HOMEï¼Œå¯ç›´æ¥ä½¿ç”¨"sumo"
    config_path = "a.sumocfg"
    ego_vehicle_id = "drl_ego_car"
    port_range = (8873, 8900)

    # è®­ç»ƒå‚æ•°
    episodes = 1000
    max_steps = 2000
    gamma = 0.99
    clip_epsilon = 0.1
    learning_rate = 3e-4
    batch_size = 256
    ppo_epochs = 3       # æ¯æ¬¡æ›´æ–°æ—¶çš„è¿­ä»£è½®æ•°
    hidden_size = 512
    log_interval = 10

    # çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
    state_dim = 10
    action_dim = 3  # 0: ä¿æŒ, 1: å·¦å˜, 2: å³å˜

# ========== SUMOç¯å¢ƒå°è£… ==========
class SumoEnv:
    def __init__(self):
        self.current_port = Config.port_range[0]
        self.sumo_process = None
        self.change_lane_count = 0
        self.collision_count = 0
        self.current_step = 0

    def _init_sumo_cmd(self, port):
        return [
            Config.sumo_binary,
            "-c", Config.config_path,
            "--remote-port", str(port),
            "--no-warnings", "true",
            "--collision.action", "none",
            "--time-to-teleport", "-1",
            "--random"
        ]

    def reset(self):
        self._close()
        self._start_sumo()
        self._add_ego_vehicle()
        self.change_lane_count = 0
        self.collision_count = 0
        self.current_step = 0
        return self._get_state()

    def _start_sumo(self):
        for port in range(*Config.port_range):
            try:
                sumo_cmd = self._init_sumo_cmd(port)
                print(f"å°è¯•è¿æ¥SUMOï¼Œç«¯å£ï¼š{port}...")
                self.sumo_process = subprocess.Popen(sumo_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                time.sleep(2)
                traci.init(port)
                print(f"âœ… SUMOè¿æ¥æˆåŠŸï¼Œç«¯å£ï¼š{port}")
                self.current_port = port
                return
            except traci.exceptions.TraCIException:
                print(f"ç«¯å£{port}è¿æ¥å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£...")
                self._kill_sumo_processes()
                time.sleep(1)
        raise ConnectionError("æ— æ³•åœ¨æŒ‡å®šç«¯å£èŒƒå›´å†…è¿æ¥SUMOï¼")

    def _add_ego_vehicle(self):
        # è‡ªå®šä¹‰ä¸€æ¡ego_routeï¼Œç”¨äºæ·»åŠ è‡ªè½¦
        if "ego_route" not in traci.route.getIDList():
            traci.route.add("ego_route", ["E0"])
        traci.vehicle.addFull(
            Config.ego_vehicle_id, "ego_route",
            typeID="car", depart="now",
            departLane="best", departSpeed="max"
        )
        for _ in range(20):
            traci.simulationStep()
            if Config.ego_vehicle_id in traci.vehicle.getIDList():
                return
        raise RuntimeError("è‡ªè½¦ç”Ÿæˆå¤±è´¥ï¼")

    def _get_state(self):
        state = np.zeros(Config.state_dim, dtype=np.float32)
        if Config.ego_vehicle_id not in traci.vehicle.getIDList():
            return state
        try:
            speed = traci.vehicle.getSpeed(Config.ego_vehicle_id)
            lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
            state[0] = speed / 33.33
            state[1] = lane / 2.0
            self._update_surrounding_vehicles(state)
            state[8] = state[1]   # å½“å‰è½¦é“
            # ç›®æ ‡è½¦é“ï¼ˆè¿™é‡Œæš‚ä¸”è®¾ä¸­é—´è½¦é“ä¸ºç›®æ ‡ï¼Œè‹¥å½“å‰å³ä¸­é—´ï¼Œåˆ™è®¾1ï¼Œå¦åˆ™0ï¼‰
            state[9] = 1.0 if lane == 1 else 0.0
        except traci.TraCIException:
            pass
        return state

    def _update_surrounding_vehicles(self, state):
        ego_pos = traci.vehicle.getPosition(Config.ego_vehicle_id)
        ego_lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
        # åˆå§‹åŒ–å„æ–¹å‘è·ç¦»ä¸ºè¾ƒå¤§å€¼(å•ä½ç±³ï¼Œåç»­è¦é™¤100å½’ä¸€åŒ–)
        ranges = {
            'front': 100.0, 'back': 100.0,
            'left_front': 100.0, 'left_back': 100.0,
            'right_front': 100.0, 'right_back': 100.0
        }
        for veh_id in traci.vehicle.getIDList():
            if veh_id == Config.ego_vehicle_id:
                continue
            veh_lane = traci.vehicle.getLaneIndex(veh_id)
            veh_pos = traci.vehicle.getPosition(veh_id)
            dx = veh_pos[0] - ego_pos[0]
            dy = veh_pos[1] - ego_pos[1]
            distance = np.hypot(dx, dy)
            if veh_lane == ego_lane:
                if dx > 0:  # å‰è½¦
                    if distance < ranges['front']:
                        ranges['front'] = distance
                else:       # åè½¦
                    if distance < ranges['back']:
                        ranges['back'] = distance
            elif veh_lane == ego_lane - 1:  # å·¦ä¾§è½¦é“
                if dx > 0:
                    if distance < ranges['left_front']:
                        ranges['left_front'] = distance
                else:
                    if distance < ranges['left_back']:
                        ranges['left_back'] = distance
            elif veh_lane == ego_lane + 1:  # å³ä¾§è½¦é“
                if dx > 0:
                    if distance < ranges['right_front']:
                        ranges['right_front'] = distance
                else:
                    if distance < ranges['right_back']:
                        ranges['right_back'] = distance
        state[2] = ranges['front'] / 100.0
        state[3] = ranges['back'] / 100.0
        state[4] = ranges['left_front'] / 100.0
        state[5] = ranges['left_back'] / 100.0
        state[6] = ranges['right_front'] / 100.0
        state[7] = ranges['right_back'] / 100.0

    def step(self, action):
        done = False
        reward = 0.0
        try:
            lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
            if action == 1 and lane > 0:
                traci.vehicle.changeLane(Config.ego_vehicle_id, lane - 1, duration=2)
                self.change_lane_count += 1
            elif action == 2 and lane < 2:
                traci.vehicle.changeLane(Config.ego_vehicle_id, lane + 1, duration=2)
                self.change_lane_count += 1

            traci.simulationStep()
            reward = self._calculate_reward(action)
            self.current_step += 1
        except traci.TraCIException:
            # è‡ªè½¦å¯èƒ½å·²è¢«ç§»é™¤(æç«¯æƒ…å†µ), ç›´æ¥done
            done = True

        next_state = self._get_state()
        if traci.simulation.getTime() > 3600 or self.current_step >= Config.max_steps:
            done = True
        return next_state, reward, done

    def _calculate_reward(self, action):
        # å¦‚æœå‘ç”Ÿç¢°æ’
        collisions = traci.simulation.getCollisions()
        if collisions:
            for collision in collisions:
                if (collision.collider == Config.ego_vehicle_id or 
                    collision.victim == Config.ego_vehicle_id):
                    self.collision_count += 1
                    return -50.0  # ç¢°æ’æƒ©ç½š

        # æ ¹æ®è‡ªè½¦é€Ÿåº¦å’Œè½¦é“ç»™äºˆå¥–åŠ±
        speed = traci.vehicle.getSpeed(Config.ego_vehicle_id)
        speed_reward = (speed / 33.33) * 0.5
        lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
        lane_reward = (2 - abs(lane - 1)) * 0.3  # å‡è®¾ä¸­é—´è½¦é“ä¼˜å…ˆ

        # å¯¹äºå˜é“ç»™äºˆå°‘é‡å¥–åŠ±ï¼Œé¼“åŠ±å°è¯•(å¯é€‰)
        change_lane_bonus = 0.1 if action != 0 else 0.0

        # å¯é€‰ï¼šè‹¥ä¸å‰è½¦éå¸¸æ¥è¿‘ï¼Œä¹Ÿç»™ä¸€ä¸ªè´Ÿå¥–åŠ±ï¼Œé¼“åŠ±ä¿æŒå®‰å…¨è·ç¦»
        front_dist = traci.vehicle.getDistance(
            Config.ego_vehicle_id, traci.vehicle.getLaneID(Config.ego_vehicle_id), 10.0, 1
        )
        # ä¸Šé¢è¿™ä¸ªAPIåªæ˜¯ç¤ºä¾‹ï¼Œä¹Ÿå¯ç”¨ _update_surrounding_vehicles é‡Œå­˜çš„ front_dist
        # è¿™é‡Œç®€å•å†™ä¸€ä¸ªï¼šå½“å‰è½¦è·ç¦»å°äº 5 ç±³ï¼Œç»™é¢å¤–æƒ©ç½š
        safe_distance_penalty = 0.0
        ego_state = self._get_state()
        front_dist_norm = ego_state[2] * 100  # front è·ç¦»çš„å®é™…ç±³æ•°
        if front_dist_norm < 5.0:
            safe_distance_penalty = -1.0

        return speed_reward + lane_reward + change_lane_bonus + safe_distance_penalty

    def _close(self):
        if self.sumo_process:
            try:
                traci.close()
            except traci.exceptions.FatalTraCIError:
                pass
            finally:
                self.sumo_process.terminate()
                self.sumo_process.wait()
                self.sumo_process = None
                self._kill_sumo_processes()

    @staticmethod
    def _kill_sumo_processes():
        if os.name == 'nt':
            os.system("taskkill /f /im sumo.exe >nul 2>&1")
            os.system("taskkill /f /im sumo-gui.exe >nul 2>&1")

# ========== PPOç®—æ³•å®ç° ==========
class PPO(nn.Module):
    def __init__(self):
        super(PPO, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(Config.state_dim, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.action_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(Config.state_dim, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, 1)
        )

    def forward(self, x):
        return self.actor(x), self.critic(x)

# ========== Agent ==========
class Agent:
    def __init__(self):
        self.policy = PPO()
        self.optimizer = optim.Adam(self.policy.parameters(), lr=Config.learning_rate)
        self.memory = []
        self.actor_losses = []
        self.critic_losses = []
        self.total_losses = []

    def get_action(self, state):
        state_tensor = torch.FloatTensor(state)
        probs, _ = self.policy(state_tensor)
        # åŠ¨ä½œå±è”½ï¼šä¸å…è®¸æœ€å·¦è½¦é“å†å·¦å˜ã€æœ€å³è½¦é“å†å³å˜
        lane = int(state[1] * 2)
        mask = [1.0] * Config.action_dim
        if lane == 0:
            mask[1] = 0.0
        elif lane == 2:
            mask[2] = 0.0
        probs = probs * torch.tensor(mask)
        probs = probs / probs.sum()  # é‡æ–°å½’ä¸€åŒ–
        dist = Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def store(self, transition):
        # transition = (state, action, log_prob, reward)
        self.memory.append(transition)

    def update(self):
        if len(self.memory) < Config.batch_size:
            # ä¸è¶³ä¸€ä¸ªbatchï¼Œå…ˆä¸æ›´æ–°
            return

        # æå–è®°å¿†
        states = torch.FloatTensor([m[0] for m in self.memory])
        actions = torch.LongTensor([m[1] for m in self.memory])
        old_log_probs = torch.FloatTensor([m[2] for m in self.memory])
        rewards = torch.FloatTensor([m[3] for m in self.memory])

        # è®¡ç®—æŠ˜æ‰£å›æŠ¥
        discounted_rewards = []
        running = 0
        for r in reversed(rewards.numpy()):
            running = r + Config.gamma * running
            discounted_rewards.insert(0, running)
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        # å½’ä¸€åŒ–
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)

        for _ in range(Config.ppo_epochs):
            new_probs, values = self.policy(states)
            dist = Categorical(new_probs)
            new_log_probs = dist.log_prob(actions)

            ratio = torch.exp(new_log_probs - old_log_probs)
            advantages = discounted_rewards - values.squeeze().detach()

            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - Config.clip_epsilon, 1 + Config.clip_epsilon) * advantages

            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = nn.MSELoss()(values.squeeze(), discounted_rewards)
            loss = actor_loss + 0.5 * critic_loss

            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()

            self.actor_losses.append(actor_loss.item())
            self.critic_losses.append(critic_loss.item())
            self.total_losses.append(loss.item())

        self.memory.clear()

# ========== è®­ç»ƒä¸»å¾ªç¯ ==========
def main():
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    results_dir = f"ppo_results_{timestamp}"
    models_dir = os.path.join(results_dir, "models")
    os.makedirs(models_dir, exist_ok=True)

    env = SumoEnv()
    agent = Agent()
    best_reward = -float('inf')

    # è®°å½•æ•°æ®
    all_rewards = []
    lane_change_counts = []
    collision_counts = []
    total_steps_per_episode = []

    try:
        for episode in tqdm(range(1, Config.episodes + 1), desc="è®­ç»ƒå›åˆ"):
            state = env.reset()
            episode_reward = 0
            done = False
            step_count = 0

            while not done and step_count < Config.max_steps:
                action, log_prob = agent.get_action(state)
                next_state, reward, done = env.step(action)
                agent.store((state, action, log_prob.item(), reward))

                state = next_state
                episode_reward += reward
                step_count += 1

            # å›åˆç»“æŸåå†ç»Ÿä¸€æ›´æ–°(æé«˜æ ·æœ¬æ•ˆç‡ï¼Œå‡å°‘å™ªå£°)
            agent.update()

            all_rewards.append(episode_reward)
            lane_change_counts.append(env.change_lane_count)
            collision_counts.append(env.collision_count)
            total_steps_per_episode.append(step_count)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if episode_reward > best_reward:
                best_reward = episode_reward
                torch.save(agent.policy.state_dict(), os.path.join(models_dir, "best_model.pth"))
                print(f"ğŸ‰ Episode {episode}, æ–°æœ€ä½³æ¨¡å‹ï¼å›åˆå¥–åŠ±ï¼š{best_reward:.2f}")

            if episode % Config.log_interval == 0:
                print(f"[Episode {episode}] Reward: {episode_reward:.2f}, Best: {best_reward:.2f}, "
                      f"LaneChange: {env.change_lane_count}, Collisions: {env.collision_count}")

    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­...")
    finally:
        env._close()
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(agent.policy.state_dict(), os.path.join(models_dir, "last_model.pth"))

        # ç»˜åˆ¶æ›²çº¿
        plt.figure(figsize=(15, 8))

        plt.subplot(2, 2, 1)
        plt.plot(all_rewards)
        plt.title("å›åˆå¥–åŠ±")

        plt.subplot(2, 2, 2)
        plt.plot(lane_change_counts)
        plt.title("å˜é“æ¬¡æ•°")

        plt.subplot(2, 2, 3)
        plt.plot(collision_counts)
        plt.title("ç¢°æ’æ¬¡æ•°")

        plt.subplot(2, 2, 4)
        plt.plot(agent.total_losses)
        plt.title("è®­ç»ƒæŸå¤±")

        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, "training_curves.png"))
        plt.close()

        np.savez(os.path.join(results_dir, "training_data.npz"),
                 rewards=all_rewards,
                 lane_changes=lane_change_counts,
                 collisions=collision_counts,
                 steps=total_steps_per_episode,
                 actor_losses=agent.actor_losses,
                 critic_losses=agent.critic_losses,
                 total_losses=agent.total_losses)

        print(f"è®­ç»ƒå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ç›®å½•: {results_dir}")

if __name__ == "__main__":
    if not (os.path.exists(Config.sumo_binary) or "SUMO_HOME" in os.environ):
        raise ValueError("SUMOè·¯å¾„é”™è¯¯ï¼Œè¯·æ£€æŸ¥SUMOæ˜¯å¦æ­£ç¡®å®‰è£…å¹¶è®¾ç½®ç¯å¢ƒå˜é‡SUMO_HOME")
    if not os.path.exists(Config.config_path):
        raise ValueError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {Config.config_path}")
    main()
