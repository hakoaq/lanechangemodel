import os
import sys
import time
import datetime
import subprocess
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import traci
from tqdm import tqdm
import matplotlib.pyplot as plt

# ========== é…ç½®åŒº ==========
class Config:
    # SUMOé…ç½®
    sumo_binary = "sumo"  # å¦‚æœå·²è®¾ç½®SUMO_HOMEï¼Œå¯ç›´æ¥ä½¿ç”¨"sumo"ï¼Œå¦åˆ™è¯·è®¾ç½®SUMOçš„ç»å¯¹è·¯å¾„
    config_path = "a.sumocfg"  # è¯·ç¡®ä¿è¯¥æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼ˆè¿™é‡Œä½¿ç”¨a.sumocfgï¼‰
    ego_vehicle_id = "drl_ego_car"  # è‡ªè½¦å”¯ä¸€æ ‡è¯†
    port_range = (8873, 8900)

    # è®­ç»ƒå‚æ•°
    episodes = 1000
    max_steps = 2000  # æ¯ä¸ªå›åˆæœ€å¤§æ­¥æ•°
    gamma = 0.995
    clip_epsilon = 0.2
    learning_rate = 1e-3
    batch_size = 256
    hidden_size = 512
    log_interval = 10

    # çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
    # çŠ¶æ€åŒ…æ‹¬ï¼š[é€Ÿåº¦, å½“å‰è½¦é“, å‰è½¦è·, åè½¦è·, å·¦å‰, å·¦å, å³å‰, å³å, å½“å‰è½¦é“ï¼ˆé‡å¤ï¼‰, ç›®æ ‡è½¦é“ï¼ˆå‡è®¾ç›®æ ‡è½¦é“ä¸ºä¸­é—´è½¦é“ï¼‰]
    state_dim = 10
    action_dim = 3  # 0: ä¿æŒ, 1: å·¦å˜, 2: å³å˜

# ========== SUMOç¯å¢ƒå°è£… ==========
class SumoEnv:
    def __init__(self):
        self.current_port = Config.port_range[0]
        self.sumo_process = None
        self.change_lane_count = 0  # è®°å½•å˜é“æ¬¡æ•°
        self.collision_count = 0    # è®°å½•ç¢°æ’æ¬¡æ•°
        self.current_step = 0       # å½“å‰æ­¥æ•°è®¡æ•°

    def _init_sumo_cmd(self, port):
        return [
            Config.sumo_binary,
            "-c", Config.config_path,
            "--remote-port", str(port),
            "--no-warnings", "true",
            "--collision.action", "none",
            "--time-to-teleport", "-1",
            "--random",
        ]

    def reset(self):
        self._close()  # é‡ç½®å‰å…³é—­ä¹‹å‰çš„SUMOè¿æ¥
        self._start_sumo()
        self._add_ego_vehicle()
        self.change_lane_count = 0
        self.collision_count = 0
        self.current_step = 0
        return self._get_state()

    def _start_sumo(self):
        for port in range(*Config.port_range):
            try:
                sumo_cmd = self._init_sumo_cmd(port)
                print(f"å°è¯•è¿æ¥SUMOï¼Œç«¯å£ï¼š{port}...")
                self.sumo_process = subprocess.Popen(sumo_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                time.sleep(2)  # ç­‰å¾…SUMOå¯åŠ¨
                traci.init(port)
                print(f"âœ… SUMOè¿æ¥æˆåŠŸï¼Œç«¯å£ï¼š{port}")
                self.current_port = port
                return
            except traci.exceptions.TraCIException:
                print(f"ç«¯å£{port}è¿æ¥å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£...")
                self._kill_sumo_processes()
                time.sleep(1)
        raise ConnectionError("æ— æ³•åœ¨æŒ‡å®šç«¯å£èŒƒå›´å†…è¿æ¥SUMOæœåŠ¡ï¼")

    def _add_ego_vehicle(self):
        if "ego_route" not in traci.route.getIDList():
            traci.route.add("ego_route", ["E0"])
        traci.vehicle.addFull(
            Config.ego_vehicle_id, "ego_route",
            typeID="car", depart="now",
            departLane="best", departSpeed="max"
        )
        # ç­‰å¾…è½¦è¾†ç”Ÿæˆ
        for _ in range(20):
            traci.simulationStep()
            if Config.ego_vehicle_id in traci.vehicle.getIDList():
                return
        raise RuntimeError("è‡ªè½¦ç”Ÿæˆå¤±è´¥ï¼")

    def _get_state(self):
        state = np.zeros(Config.state_dim, dtype=np.float32)
        if Config.ego_vehicle_id not in traci.vehicle.getIDList():
            return state
        try:
            # åŸºç¡€çŠ¶æ€ä¿¡æ¯
            speed = traci.vehicle.getSpeed(Config.ego_vehicle_id)
            lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
            state[0] = speed / 33.33  # é€Ÿåº¦å½’ä¸€åŒ–
            state[1] = lane / 2.0     # è½¦é“å½’ä¸€åŒ–ï¼ˆå‡è®¾è½¦é“ç´¢å¼•0~2ï¼‰
            self._update_surrounding_vehicles(state)
            # å½“å‰è½¦é“åŠç›®æ ‡è½¦é“ï¼ˆå‡è®¾ç›®æ ‡è½¦é“ä¸ºä¸­é—´è½¦é“ï¼Œç´¢å¼•1ï¼‰
            state[8] = state[1]
            state[9] = 1.0 if lane == 1 else 0.0
        except traci.TraCIException:
            pass
        return state

    def _update_surrounding_vehicles(self, state):
        try:
            ego_pos = traci.vehicle.getPosition(Config.ego_vehicle_id)
            ego_lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
            # åˆå§‹åŒ–å„æ–¹å‘è·ç¦»ï¼Œåˆå€¼ä¸ºè¾ƒå¤§æ•°å€¼ï¼ˆå½’ä¸€åŒ–æ—¶ä»¥100mä¸ºåŸºå‡†ï¼‰
            ranges = {
                'front': (100.0, None), 'back': (100.0, None),
                'left_front': (100.0, None), 'left_back': (100.0, None),
                'right_front': (100.0, None), 'right_back': (100.0, None)
            }
            for veh_id in traci.vehicle.getIDList():
                if veh_id == Config.ego_vehicle_id:
                    continue
                veh_lane = traci.vehicle.getLaneIndex(veh_id)
                veh_pos = traci.vehicle.getPosition(veh_id)
                dx = veh_pos[0] - ego_pos[0]
                dy = veh_pos[1] - ego_pos[1]
                distance = np.hypot(dx, dy)
                # æ ¹æ®è½¦é“å…³ç³»åˆ¤æ–­
                if veh_lane == ego_lane - 1:
                    key = 'left_front' if dx > 0 else 'left_back'
                elif veh_lane == ego_lane + 1:
                    key = 'right_front' if dx > 0 else 'right_back'
                elif veh_lane == ego_lane:
                    key = 'front' if dx > 0 else 'back'
                else:
                    continue
                if distance < ranges[key][0]:
                    ranges[key] = (distance, veh_id)
            state[2] = ranges['front'][0] / 100.0
            state[3] = ranges['back'][0] / 100.0
            state[4] = ranges['left_front'][0] / 100.0
            state[5] = ranges['left_back'][0] / 100.0
            state[6] = ranges['right_front'][0] / 100.0
            state[7] = ranges['right_back'][0] / 100.0
        except traci.TraCIException:
            print("è­¦å‘Šï¼šæ›´æ–°å‘¨å›´è½¦è¾†çŠ¶æ€å‡ºé”™ã€‚")

    def step(self, action):
        reward = 0.0
        done = False
        try:
            lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
            # æ ¹æ®åŠ¨ä½œè¿›è¡Œå˜é“æ“ä½œï¼ˆ1ï¼šå·¦å˜ï¼Œ2ï¼šå³å˜ï¼‰
            if action == 1 and lane > 0:
                traci.vehicle.changeLane(Config.ego_vehicle_id, lane - 1, duration=2)
                self.change_lane_count += 1
            elif action == 2 and lane < 2:
                traci.vehicle.changeLane(Config.ego_vehicle_id, lane + 1, duration=2)
                self.change_lane_count += 1

            traci.simulationStep()
            reward = self._calculate_reward(action)
            self.current_step += 1
        except traci.TraCIException:
            done = True

        next_state = self._get_state()
        # ç»“æŸæ¡ä»¶ï¼šä»¿çœŸæ—¶é—´è¶…è¿‡3600ç§’æˆ–æ­¥æ•°è¾¾åˆ°ä¸Šé™
        done = done or traci.simulation.getTime() > 3600 or self.current_step >= Config.max_steps
        return next_state, reward, done

    def _calculate_reward(self, action):
        # æ£€æŸ¥ç¢°æ’
        if traci.simulation.getCollisions():
            for collision in traci.simulation.getCollisions():
                if collision.collider == Config.ego_vehicle_id or collision.victim == Config.ego_vehicle_id:
                    self.collision_count += 1
                    return -50.0  # ç¢°æ’ç»™äºˆè¾ƒå¤§æƒ©ç½š

        # å®‰å…¨é«˜æ•ˆå¥–åŠ±è®¾è®¡ï¼š
        speed = traci.vehicle.getSpeed(Config.ego_vehicle_id)
        speed_reward = (speed / 33.33) * 0.5  # é€Ÿåº¦è¶Šå¿«å¥–åŠ±è¶Šé«˜ï¼ˆå½’ä¸€åŒ–åä¹˜ä»¥æƒé‡ï¼‰
        lane = traci.vehicle.getLaneIndex(Config.ego_vehicle_id)
        # é¼“åŠ±å¤„äºä¸­é—´è½¦é“ï¼ˆå‡è®¾ä¸­é—´è½¦é“ä¸ºç›®æ ‡è½¦é“ï¼‰
        lane_reward = (2 - abs(lane - 1)) * 0.3
        # è‹¥æ‰§è¡Œå˜é“åŠ¨ä½œï¼Œç»™äºˆé¢å¤–å¥–åŠ±ä»¥é¼“åŠ±å˜é“ï¼Œä½†å‰ææ˜¯æœªå‘ç”Ÿç¢°æ’
        change_lane_bonus = 0.2 if action != 0 else 0.0
        return speed_reward + lane_reward + change_lane_bonus

    def _close(self):
        if self.sumo_process:
            try:
                traci.close()
            except traci.exceptions.FatalTraCIError:
                pass
            finally:
                self.sumo_process.terminate()
                self.sumo_process.wait()
                self.sumo_process = None
                self._kill_sumo_processes()

    @staticmethod
    def _kill_sumo_processes():
        if os.name == 'nt':
            os.system("taskkill /f /im sumo.exe >nul 2>&1")
            os.system("taskkill /f /im sumo-gui.exe >nul 2>&1")

# ========== PPOç®—æ³•å®ç° ==========
class PPO(nn.Module):
    def __init__(self):
        super(PPO, self).__init__()
        self.actor = nn.Sequential(
            nn.Linear(Config.state_dim, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.action_dim),
            nn.Softmax(dim=-1)
        )
        self.critic = nn.Sequential(
            nn.Linear(Config.state_dim, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, Config.hidden_size),
            nn.ReLU(),
            nn.Linear(Config.hidden_size, 1)
        )

    def forward(self, x):
        return self.actor(x), self.critic(x)

# ========== Agent ==========
class Agent:
    def __init__(self):
        self.policy = PPO()
        self.optimizer = optim.Adam(self.policy.parameters(), lr=Config.learning_rate)
        self.memory = []
        # ä¸ºè¿½è¸ªè®­ç»ƒæŸå¤±
        self.actor_losses = []
        self.critic_losses = []
        self.total_losses = []

    def get_action(self, state):
        state_tensor = torch.FloatTensor(state)
        probs, _ = self.policy(state_tensor)
        # é‡‡ç”¨åŠ¨ä½œå±è”½ä¿è¯å˜é“åˆæ³•æ€§
        lane = int(state[1] * 2)  # å°†å½’ä¸€åŒ–åçš„è½¦é“è¿˜åŸ
        mask = [1.0] * Config.action_dim
        if lane == 0:  # æœ€å·¦ä¾§è½¦é“ï¼Œä¸èƒ½å·¦å˜
            mask[1] = 0.0
        elif lane == 2:  # æœ€å³ä¾§è½¦é“ï¼Œä¸èƒ½å³å˜
            mask[2] = 0.0
        probs = probs * torch.tensor(mask)
        probs = probs / probs.sum()  # é‡æ–°å½’ä¸€åŒ–
        dist = Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

    def update(self):
        if len(self.memory) < Config.batch_size:
            return {"actor_loss": 0, "critic_loss": 0, "total_loss": 0}

        states = torch.FloatTensor([t[0] for t in self.memory])
        actions = torch.LongTensor([t[1] for t in self.memory])
        old_log_probs = torch.FloatTensor([t[2] for t in self.memory])
        rewards = torch.FloatTensor([t[3] for t in self.memory])

        # è®¡ç®—æŠ˜æ‰£å›æŠ¥
        discounted_rewards = []
        running_reward = 0
        for r in reversed(rewards.numpy()):
            running_reward = r + Config.gamma * running_reward
            discounted_rewards.insert(0, running_reward)
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)

        total_actor_loss = 0
        total_critic_loss = 0
        total_loss = 0

        # PPOå¤šæ­¥æ›´æ–°
        for _ in range(5):
            new_probs, values = self.policy(states)
            dist = Categorical(new_probs)
            new_log_probs = dist.log_prob(actions)
            ratio = torch.exp(new_log_probs - old_log_probs)
            advantages = discounted_rewards - values.squeeze().detach()
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - Config.clip_epsilon, 1 + Config.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = nn.MSELoss()(values.squeeze(), discounted_rewards)
            loss = actor_loss + 0.5 * critic_loss

            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
            self.optimizer.step()

            total_actor_loss += actor_loss.item()
            total_critic_loss += critic_loss.item()
            total_loss += loss.item()

        avg_actor_loss = total_actor_loss / 5
        avg_critic_loss = total_critic_loss / 5
        avg_total_loss = total_loss / 5

        self.actor_losses.append(avg_actor_loss)
        self.critic_losses.append(avg_critic_loss)
        self.total_losses.append(avg_total_loss)

        self.memory.clear()

        return {
            "actor_loss": avg_actor_loss,
            "critic_loss": avg_critic_loss,
            "total_loss": avg_total_loss
        }

# ========== è®­ç»ƒä¸»å¾ªç¯ ==========
def main():
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    results_dir = f"ppo_results_{timestamp}"
    models_dir = os.path.join(results_dir, "models")
    os.makedirs(models_dir, exist_ok=True)

    env = SumoEnv()
    agent = Agent()
    best_reward = -float('inf')

    # ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹æ•°æ®
    episode_rewards = []
    episode_steps = []
    episode_lane_changes = []
    episode_collisions = []
    losses = []

    try:
        for episode in tqdm(range(1, Config.episodes + 1), desc="è®­ç»ƒå›åˆ"):
            state = env.reset()
            episode_reward = 0
            done = False
            step_count = 0

            while not done and step_count < Config.max_steps:
                action, log_prob = agent.get_action(state)
                next_state, reward, done = env.step(action)
                agent.memory.append((state, action, log_prob, reward))
                episode_reward += reward
                state = next_state
                step_count += 1

                if len(agent.memory) >= Config.batch_size:
                    loss_info = agent.update()
                    losses.append(loss_info["total_loss"])

            # æœ€åå‰©ä½™çš„memoryæ›´æ–°
            if len(agent.memory) > 0:
                loss_info = agent.update()
                if loss_info["total_loss"] > 0:
                    losses.append(loss_info["total_loss"])

            episode_rewards.append(episode_reward)
            episode_steps.append(env.current_step)
            episode_lane_changes.append(env.change_lane_count)
            episode_collisions.append(env.collision_count)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if episode_reward > best_reward:
                best_reward = episode_reward
                torch.save(agent.policy.state_dict(), os.path.join(models_dir, "best_model.pth"))
                print(f"ğŸ‰ æ–°æœ€ä½³æ¨¡å‹ï¼å›åˆå¥–åŠ±ï¼š{best_reward:.2f}")

            if episode % Config.log_interval == 0:
                print(f"Episode {episode}, Reward: {episode_reward:.2f}, Best: {best_reward:.2f}, "
                      f"Lane Changes: {env.change_lane_count}, Collisions: {env.collision_count}")
    except KeyboardInterrupt:
        print("è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­")
    finally:
        env._close()
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save(agent.policy.state_dict(), os.path.join(models_dir, "last_model.pth"))

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        plt.figure(figsize=(15, 10))

        plt.subplot(2, 2, 1)
        plt.plot(episode_rewards)
        plt.title("å›åˆå¥–åŠ±")
        plt.xlabel("å›åˆ")
        plt.ylabel("å¥–åŠ±")

        plt.subplot(2, 2, 2)
        plt.plot(episode_lane_changes)
        plt.title("å˜é“æ¬¡æ•°")
        plt.xlabel("å›åˆ")
        plt.ylabel("æ¬¡æ•°")

        plt.subplot(2, 2, 3)
        plt.plot(losses)
        plt.title("è®­ç»ƒæŸå¤±")
        plt.xlabel("æ›´æ–°æ¬¡æ•°")
        plt.ylabel("æŸå¤±")

        plt.subplot(2, 2, 4)
        plt.plot(episode_collisions)
        plt.title("ç¢°æ’æ¬¡æ•°")
        plt.xlabel("å›åˆ")
        plt.ylabel("æ¬¡æ•°")

        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, "training_curves.png"))
        plt.close()

        # ä¿å­˜è®­ç»ƒæ•°æ®
        np.savez(os.path.join(results_dir, "training_data.npz"),
                 rewards=episode_rewards,
                 lane_changes=episode_lane_changes,
                 steps=episode_steps,
                 collisions=episode_collisions,
                 actor_losses=agent.actor_losses,
                 critic_losses=agent.critic_losses,
                 total_losses=agent.total_losses)

        print(f"è®­ç»ƒå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ç›®å½•: {results_dir}")

if __name__ == "__main__":
    if not (os.path.exists(Config.sumo_binary) or "SUMO_HOME" in os.environ):
        raise ValueError("SUMOè·¯å¾„é”™è¯¯ï¼Œè¯·æ£€æŸ¥SUMOæ˜¯å¦æ­£ç¡®å®‰è£…å¹¶è®¾ç½®ç¯å¢ƒå˜é‡SUMO_HOMEï¼Œæˆ–åœ¨Configä¸­è®¾ç½®æ­£ç¡®çš„è·¯å¾„")
    if not os.path.exists(Config.config_path):
        raise ValueError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{Config.config_path}")
    main()
