# -*- coding: utf-8 -*-
import os
import sys
import numpy as np
import random
import time
import matplotlib.pyplot as plt
from collections import deque
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import datetime
import traci
import sumolib

# è®¾ç½®GPUå†…å­˜åŠ¨æ€å¢é•¿
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# æ£€æŸ¥SUMO_HOMEç¯å¢ƒå˜é‡
if 'SUMO_HOME' not in os.environ:
    sys.exit("è¯·è®¾ç½®ç¯å¢ƒå˜é‡'SUMO_HOME'")
tools = os.path.join(os.environ['SUMO_HOME'], 'tools')
sys.path.append(tools)

# SUMOé…ç½®
SUMO_CONFIG_PATH = "a.sumocfg"
SUMO_BINARY = sumolib.checkBinary("sumo")  # ä½¿ç”¨"sumo-gui"å¯è¿›è¡Œå¯è§†åŒ–


# DQNæ™ºèƒ½ä½“ï¼ˆä¸PPOå‚æ•°ä¸€è‡´ç‰ˆæœ¬ï¼‰
class LaneChangeDQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=50000)
        self.gamma = 0.995  # ä¸PPOä¸€è‡´çš„æŠ˜æ‰£ç‡
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001  # ä¸PPOä¸€è‡´çš„å­¦ä¹ ç‡
        self.batch_size = 256  # ä¸PPOä¸€è‡´çš„æ‰¹é‡å¤§å°
        self.train_start = 1000
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()
        self.update_count = 0
        self.update_frequency = 10

    def _build_model(self):
        # ä½¿ç”¨ä¸PPOç›¸åŒçš„ç½‘ç»œç»“æ„ï¼š3å±‚512ä¸ªç¥ç»å…ƒ
        model = Sequential()
        model.add(Dense(512, input_dim=self.state_size, activation='relu'))
        model.add(Dense(512, activation='relu'))
        model.add(Dense(512, activation='relu'))  # æ·»åŠ ç¬¬ä¸‰å±‚ï¼Œä¸PPOä¸€è‡´
        model.add(Dense(self.action_size, activation='linear'))
        
        # ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡ï¼Œä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate))
        return model

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, training=True):
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        act_values = self.model.predict(state, verbose=0)
        return np.argmax(act_values[0])

    def replay(self):
        if len(self.memory) < self.train_start:
            return 0
        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))
        states = np.zeros((len(minibatch), self.state_size))
        next_states = np.zeros((len(minibatch), self.state_size))
        for i, (state, _, _, next_state, _) in enumerate(minibatch):
            states[i] = state
            next_states[i] = next_state
        targets = self.model.predict(states, verbose=0)
        next_state_values = self.target_model.predict(next_states, verbose=0)
        for i, (state, action, reward, next_state, done) in enumerate(minibatch):
            targets[i][action] = reward if done else reward + self.gamma * np.max(next_state_values[i])
        history = self.model.fit(states, targets, epochs=1, verbose=0, batch_size=self.batch_size)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        self.update_count += 1
        if self.update_count % self.update_frequency == 0:
            self.update_target_model()

        return history.history['loss'][0]

    def save(self, name):
        if not name.endswith('.keras'):
            name += '.keras'
        self.model.save(name)


# SUMOç¯å¢ƒï¼ˆä¸PPOä¸€è‡´çš„å¥–åŠ±ç»“æ„ï¼‰
class SUMOEnvironment:
    def __init__(self, ego_vehicle_id, sumo_config_path, sumo_binary):
        self.ego_vehicle_id = ego_vehicle_id
        self.sumo_config_path = sumo_config_path
        self.sumo_binary = sumo_binary
        self.max_steps = 2000  # å¢åŠ æ­¥æ•°ï¼Œä½†ä¸è¦åƒPPOé‚£æ ·åªç”¨2æ­¥ï¼Œé‚£å¯èƒ½æ˜¯ä¸ªé”™è¯¯
        self.current_step = 0
        self.sim_step_length = 0.1
        self.state_size = 10  # ä¸PPOä¸€è‡´çš„çŠ¶æ€ç»´åº¦
        self.action_size = 3
        self.last_action = 0
        self.change_lane_count = 0  # è®°å½•å˜é“æ¬¡æ•°
        self.collision_count = 0    # è®°å½•æ’è½¦æ¬¡æ•°

    def start_simulation(self):
        sumo_cmd = [self.sumo_binary, "-c", self.sumo_config_path, "--start", "--step-length",
                    str(self.sim_step_length), "--no-warnings", "true",
                    "--collision.action", "none", "--time-to-teleport", "-1", "--random"]
        traci.start(sumo_cmd)
        traci.route.add("ego_route", ["E0"])
        traci.vehicle.add(self.ego_vehicle_id, "ego_route", typeID="car")
        traci.vehicle.setSpeedMode(self.ego_vehicle_id, 31)
        traci.vehicle.setLaneChangeMode(self.ego_vehicle_id, 0)
        traci.vehicle.moveTo(self.ego_vehicle_id, "E0_1", 5)  # èµ·å§‹åœ¨ä¸­é—´è½¦é“
        traci.vehicle.setSpeed(self.ego_vehicle_id, 25)
        traci.vehicle.subscribeContext(self.ego_vehicle_id, traci.constants.CMD_GET_VEHICLE_VARIABLE, 100)
        for _ in range(20):
            traci.simulationStep()

    def reset(self):
        if traci.isLoaded():
            traci.close()
        self.start_simulation()
        self.current_step = 0
        self.last_action = 0
        self.change_lane_count = 0
        self.collision_count = 0
        return self._get_state()

    def _get_state(self):
        """è·å–è½¦è¾†çŠ¶æ€ï¼Œä¸PPOä½¿ç”¨ç›¸åŒçš„çŠ¶æ€è¡¨ç¤º"""
        state = np.zeros(self.state_size, dtype=np.float32)
        try:
            # åŸºæœ¬çŠ¶æ€
            speed = traci.vehicle.getSpeed(self.ego_vehicle_id)
            lane = traci.vehicle.getLaneIndex(self.ego_vehicle_id)
            state[0] = speed / 33.33  # å½’ä¸€åŒ–é€Ÿåº¦
            state[1] = lane / 2.0  # å½’ä¸€åŒ–è½¦é“ç´¢å¼•

            # å‘¨å›´è½¦è¾†ä¿¡æ¯
            ego_pos = traci.vehicle.getPosition(self.ego_vehicle_id)

            # åˆå§‹åŒ–è·ç¦»å€¼
            ranges = {
                'front': 100.0,
                'back': 100.0,
                'left_front': 100.0,
                'left_back': 100.0,
                'right_front': 100.0,
                'right_back': 100.0
            }

            # æ£€æµ‹å‘¨å›´è½¦è¾†
            surrounding_vehicles = traci.vehicle.getContextSubscriptionResults(self.ego_vehicle_id)
            if surrounding_vehicles:
                for v_id, v_data in surrounding_vehicles.items():
                    if v_id != self.ego_vehicle_id:
                        v_lane = traci.vehicle.getLaneIndex(v_id)
                        v_pos = traci.vehicle.getPosition(v_id)
                        # è®¡ç®—ç›¸å¯¹ä½ç½®
                        dx = v_pos[0] - ego_pos[0]  # çºµå‘è·ç¦»
                        dy = v_pos[1] - ego_pos[1]  # æ¨ªå‘è·ç¦»
                        distance = np.hypot(dx, dy)

                        # ç¡®å®šè½¦è¾†ç›¸å¯¹ä½ç½®
                        if v_lane == lane - 1:  # å·¦ä¾§è½¦é“
                            if dx > 0:  # å‰æ–¹
                                ranges['left_front'] = min(ranges['left_front'], distance)
                            else:  # åæ–¹
                                ranges['left_back'] = min(ranges['left_back'], distance)
                        elif v_lane == lane + 1:  # å³ä¾§è½¦é“
                            if dx > 0:  # å‰æ–¹
                                ranges['right_front'] = min(ranges['right_front'], distance)
                            else:  # åæ–¹
                                ranges['right_back'] = min(ranges['right_back'], distance)
                        elif v_lane == lane:  # åŒè½¦é“
                            if dx > 0:  # å‰æ–¹
                                ranges['front'] = min(ranges['front'], distance)
                            else:  # åæ–¹
                                ranges['back'] = min(ranges['back'], distance)

            # å½’ä¸€åŒ–è·ç¦»å¹¶è®¾ç½®çŠ¶æ€
            state[2] = ranges['front'] / 100.0
            state[3] = ranges['back'] / 100.0
            state[4] = ranges['left_front'] / 100.0
            state[5] = ranges['left_back'] / 100.0
            state[6] = ranges['right_front'] / 100.0
            state[7] = ranges['right_back'] / 100.0

            # å½“å‰è½¦é“ä¿¡æ¯å’Œç›®æ ‡è½¦é“ä¿¡æ¯ï¼ˆä¸PPOä¸€è‡´ï¼‰
            state[8] = lane / 2.0  # å½“å‰è½¦é“ï¼ˆå½’ä¸€åŒ–ï¼‰
            state[9] = 1.0 if lane == 1 else 0.0  # æ˜¯å¦åœ¨ä¸­é—´è½¦é“

        except:
            pass

        return state.reshape(1, self.state_size)

    def step(self, action):
        if self.ego_vehicle_id not in traci.vehicle.getIDList():
            return np.zeros((1, self.state_size)), -10, True, {}  # ä½¿ç”¨ä¸PPOä¸€è‡´çš„ç¢°æ’æƒ©ç½š

        old_lane = traci.vehicle.getLaneIndex(self.ego_vehicle_id)
        old_speed = traci.vehicle.getSpeed(self.ego_vehicle_id)
        collision = False

        try:
            # æ‰§è¡Œè½¦é“å˜æ›´ï¼Œä½¿ç”¨duration=2ä¸PPOä¸€è‡´
            if action == 1 and old_lane > 0:  # å·¦å˜é“
                traci.vehicle.changeLane(self.ego_vehicle_id, old_lane - 1, 2)
                self.change_lane_count += 1
            elif action == 2 and old_lane < 2:  # å³å˜é“
                traci.vehicle.changeLane(self.ego_vehicle_id, old_lane + 1, 2)
                self.change_lane_count += 1

            # æ§åˆ¶é€Ÿåº¦
            current_speed = traci.vehicle.getSpeed(self.ego_vehicle_id)
            lane_id = traci.vehicle.getLaneID(self.ego_vehicle_id)
            desired_speed = min(traci.lane.getMaxSpeed(lane_id), current_speed + 1)
            traci.vehicle.setSpeed(self.ego_vehicle_id, desired_speed)

            # åªæ‰§è¡Œä¸€æ­¥æ¨¡æ‹Ÿï¼Œä¸PPOä¸€è‡´
            traci.simulationStep()

            # æ£€æŸ¥ç¢°æ’
            if traci.simulation.getCollisions():
                for collision_data in traci.simulation.getCollisions():
                    if collision_data.collider == self.ego_vehicle_id or collision_data.victim == self.ego_vehicle_id:
                        collision = True
                        self.collision_count += 1
                        break
            
            if self.ego_vehicle_id not in traci.vehicle.getIDList():
                collision = True

        except:
            collision = True

        next_state = self._get_state()

        # è·å–å½“å‰çŠ¶æ€
        if self.ego_vehicle_id in traci.vehicle.getIDList():
            new_lane = traci.vehicle.getLaneIndex(self.ego_vehicle_id)
            new_speed = traci.vehicle.getSpeed(self.ego_vehicle_id)
        else:
            new_lane = old_lane
            new_speed = 0

        # è®¡ç®—å¥–åŠ±ï¼Œä½¿ç”¨ä¸PPOä¸€è‡´çš„å¥–åŠ±ç»“æ„
        reward = self._calculate_reward(action, old_lane, new_lane, new_speed, collision)

        self.current_step += 1
        done = collision or self.current_step >= self.max_steps or (
                self.ego_vehicle_id in traci.vehicle.getIDList() and
                traci.vehicle.getLanePosition(self.ego_vehicle_id) > 900
        )

        # ç”¨äºè°ƒè¯•
        info = {
            'old_lane': old_lane,
            'new_lane': new_lane,
            'old_speed': old_speed,
            'new_speed': new_speed,
            'collision': collision,
            'step': self.current_step,
            'change_lane_count': self.change_lane_count,
            'collision_count': self.collision_count
        }

        self.last_action = action
        return next_state, reward, done, info

    def _calculate_reward(self, action, old_lane, new_lane, speed, collision):
        """ä½¿ç”¨ä¸PPOä¸€è‡´çš„å¥–åŠ±è®¡ç®—æ–¹å¼"""
        if collision:
            return -10.0  # ç¢°æ’æƒ©ç½šï¼Œä¸PPOä¸€è‡´

        reward = 0.0

        # é€Ÿåº¦å¥–åŠ±
        reward += (speed / 33.33) * 0.5  # ä¸PPOä¸€è‡´çš„é€Ÿåº¦å¥–åŠ±æƒé‡

        # è½¦é“ä½ç½®å¥–åŠ±ï¼ˆé¼“åŠ±åœ¨ä¸­é—´è½¦é“ï¼‰
        reward += (2 - abs(new_lane - 1)) * 0.3  # ä¸PPOä¸€è‡´çš„è½¦é“å¥–åŠ±æƒé‡

        # å˜é“å¥–åŠ±
        if action != 0:  # å¦‚æœå°è¯•å˜é“
            reward += 0.2  # ä¸PPOä¸€è‡´çš„å˜é“å¥–åŠ±

        return reward

    def close(self):
        if traci.isLoaded():
            traci.close()


# è®­ç»ƒå‡½æ•°
def train_lane_change_agent(episodes=1000):  # æ”¹ä¸º20å›åˆï¼Œä¸PPOä¸€è‡´
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    results_dir = f"dqn_results_{timestamp}"
    models_dir = f"{results_dir}/models"
    os.makedirs(models_dir, exist_ok=True)

    env = SUMOEnvironment("ego_vehicle", SUMO_CONFIG_PATH, SUMO_BINARY)
    agent = LaneChangeDQNAgent(env.state_size, env.action_size)

    start_time = time.time()
    episode_rewards = []
    episode_steps = []
    episode_lane_changes = []
    train_info = {
        'loss': [],
        'epsilon': []
    }
    best_reward = -float('inf')  # è®°å½•æœ€ä½³å¥–åŠ±ï¼Œä¸PPOä¸€è‡´

    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        total_loss = 0
        loss_count = 0
        done = False

        while not done:
            action = agent.act(state)
            next_state, reward, done, info = env.step(action)

            agent.remember(state[0], action, reward, next_state[0], done)
            loss = agent.replay()

            if loss > 0:  # åªæœ‰å½“è®­ç»ƒå‘ç”Ÿæ—¶æ‰è®°å½•æŸå¤±
                total_loss += loss
                loss_count += 1

            state = next_state
            total_reward += reward

        avg_loss = total_loss / max(1, loss_count)
        episode_rewards.append(total_reward)
        episode_steps.append(env.current_step)
        episode_lane_changes.append(env.change_lane_count)
        train_info['loss'].append(avg_loss)
        train_info['epsilon'].append(agent.epsilon)

        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œä¸PPOä¸€è‡´
        if total_reward > best_reward:
            best_reward = total_reward
            agent.save(f"{models_dir}/best_model")
            print(f"ğŸ‰ å‘ç°æ–°æœ€ä½³æ¨¡å‹ï¼å¥–åŠ±ï¼š{best_reward:.2f}")

        # æ¯10å›åˆä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼Œä¸PPOçš„log_intervalä¸€è‡´
        if (episode + 1) % 10 == 0:
            agent.save(f"{models_dir}/dqn_model_{episode + 1}")
            print(f"Episode {episode + 1}, å¥–åŠ±ï¼š{total_reward:.2f}, æœ€ä½³ï¼š{best_reward:.2f}, "
                  f"å˜é“æ¬¡æ•°ï¼š{env.change_lane_count}, æ’è½¦æ¬¡æ•°ï¼š{env.collision_count}")

        # è®¡ç®—æ—¶é—´å’Œè¿›åº¦
        elapsed_time = time.time() - start_time
        estimated_total_time = elapsed_time / (episode + 1) * episodes
        remaining_time = estimated_total_time - elapsed_time
        progress = (episode + 1) / episodes
        bar = '#' * int(20 * progress) + '-' * (20 - int(20 * progress))

        # æ—¥å¿—è¾“å‡º
        print(
            f"å›åˆ {episode + 1}/{episodes} | å¥–åŠ±: {total_reward:.2f} | å¹³å‡æŸå¤±: {avg_loss:.4f} | è½¦é“å˜æ›´: {env.change_lane_count}")
        print(
            f"[{bar}] {progress * 100:.1f}% | å·²ç”¨: {elapsed_time / 60:.2f} åˆ†é’Ÿ | å‰©ä½™: {remaining_time / 60:.2f} åˆ†é’Ÿ")

    env.close()
    agent.save(f"{models_dir}/last_model")  # ä¿å­˜æœ€åçš„æ¨¡å‹ï¼Œä¸PPOä¸€è‡´

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(15, 10))

    plt.subplot(2, 2, 1)
    plt.plot(episode_rewards)
    plt.title("å›åˆå¥–åŠ±")
    plt.xlabel("å›åˆ")
    plt.ylabel("å¥–åŠ±")

    plt.subplot(2, 2, 2)
    plt.plot(episode_lane_changes)
    plt.title("è½¦é“å˜æ›´æ¬¡æ•°")
    plt.xlabel("å›åˆ")
    plt.ylabel("å˜æ›´æ¬¡æ•°")

    plt.subplot(2, 2, 3)
    plt.plot(train_info['loss'])
    plt.title("è®­ç»ƒæŸå¤±")
    plt.xlabel("å›åˆ")
    plt.ylabel("æŸå¤±")

    plt.subplot(2, 2, 4)
    plt.plot(train_info['epsilon'])
    plt.title("æ¢ç´¢ç‡")
    plt.xlabel("å›åˆ")
    plt.ylabel("Epsilon")

    plt.tight_layout()
    plt.savefig(f"{results_dir}/training_curves.png")
    plt.close()

    # ä¿å­˜è®­ç»ƒæ•°æ®
    np.savez(f"{results_dir}/training_data.npz",
             rewards=episode_rewards,
             lane_changes=episode_lane_changes,
             steps=episode_steps,
             loss=train_info['loss'],
             epsilon=train_info['epsilon'])

    print(f"è®­ç»ƒå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {results_dir}")
    return agent, episode_rewards, episode_lane_changes


if __name__ == "__main__":
    np.random.seed(42)
    tf.random.set_seed(42)
    random.seed(42)

    print("å¼€å§‹è®­ç»ƒDQNæ¨¡å‹...")
    agent, rewards, lane_changes = train_lane_change_agent(episodes=1000)  # è®¾ç½®ä¸º20å›åˆï¼Œä¸PPOä¸€è‡´
    print("è®­ç»ƒå®Œæˆï¼")
